from flask import Flask, request, jsonify
import os
import json
import subprocess
import threading
import shutil
import io
import base64
import requests
import random
import numpy as np
import cv2
import torch
import boto3
import matplotlib.pyplot as plt
import svgwrite
import cairosvg
from PIL import Image
from dotenv import load_dotenv
from sklearn.cluster import KMeans
from sklearn.neighbors import kneighbors_graph
from scipy.sparse.csgraph import minimum_spanning_tree, connected_components
from sklearn.metrics import pairwise_distances

# Pinecone & OpenAI
import pinecone
from pinecone import Pinecone
import openai
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI

# Diffusers & Segmentation Models
from diffusers import DiffusionPipeline, UNet2DConditionModel
from segment_anything import sam_model_registry, SamPredictor

# AWS
from botocore.client import Config

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# AWS í™˜ê²½ë³€ìˆ˜
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
BUCKET_NAME = os.getenv("BUCKET_NAME")

# dreambooth
MODEL_NAME = "runwayml/stable-diffusion-v1-5" 
TRAIN_SCRIPT = "./train_dreambooth.py"

# open_ai & Pinecone
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

# ì¥ë¡€ì‹ì¥ ì •ë³´ JSON ë¡œë“œ
FUNERAL_JSON_PATH = "./funeral_service.json"
def load_json_data(file_path):
    """JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return json.load(file)
    except Exception as e:
        print(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []
funeral_data = load_json_data(FUNERAL_JSON_PATH)

# Pidinet
# sam_checkpoint = "./sam_vit_b_01ec64.pth"  # SAM checkpoint íŒŒì¼ ê²½ë¡œ
# model_type = "vit_b"
# state_dict = torch.load(sam_checkpoint, weights_only=True)
# sam = sam_model_registry[model_type](checkpoint=None)  # Load without unsafe data
# sam.load_state_dict(state_dict)
# predictor = SamPredictor(sam)

# Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=PINECONE_API_KEY)
# Pinecone ì¸ë±ìŠ¤ ì—°ê²°
index_meritz = pc.Index("meritz")
index_samsung = pc.Index("samsung")
index_hanhwa = pc.Index("hanwha")
index_diagnostic = pc.Index("diagnostic")

training_status = {"status": "idle"}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

app = Flask(__name__)

# S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    config=Config(signature_version='s3v4')
)

def download_s3_images(image_urls, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    
    for i, url in enumerate(image_urls):
        https_url = url  # S3 URLì„ HTTPSë¡œ ë³€í™˜
        try:
            response = requests.get(https_url, stream=True)
            if response.status_code == 200:
                save_path = os.path.join(save_dir, f"image_{i}.jpg")
                with open(save_path, "wb") as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
                print(f"âœ… Downloaded: {https_url} -> {save_path}")
            else:
                print(f"âŒ Failed to download {https_url} (Status Code: {response.status_code})")
        except Exception as e:
            print(f"âŒ Error downloading {https_url}: {e}")

def stars_download_s3_images(image_urls, save_folder="./img"):
    """
    HTTP(S) í˜•ì‹ì˜ S3 ì´ë¯¸ì§€ URL ëª©ë¡ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ í´ë”ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜.

    :param image_urls: S3ì—ì„œ ì œê³µí•˜ëŠ” ì´ë¯¸ì§€ì˜ HTTP URL ë¦¬ìŠ¤íŠ¸
    :param save_folder: ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ë¡œì»¬ í´ë” (ê¸°ë³¸ê°’: ./img)
    :return: ì €ì¥ëœ ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)

    # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(image_urls, str):
        image_urls = [image_urls]

    if not os.path.exists(save_folder):
        os.makedirs(save_folder)

    saved_paths = []
    for image_url in image_urls:
        filename = os.path.basename(image_url)
        save_path = os.path.join(save_folder, filename)

        try:
            response = requests.get(image_url, stream=True)
            response.raise_for_status()  # HTTP ì—ëŸ¬ ì²˜ë¦¬

            with open(save_path, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)

            print(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path}")
            saved_paths.append(save_path)

        except requests.exceptions.RequestException as e:
            print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}, ì˜¤ë¥˜: {e}")
    
    return saved_paths

def upload_svg_to_s3(bucket_name, object_name=None):
    """
    .svg íŒŒì¼ì„ S3ì— ì—…ë¡œë“œí•˜ê³  URLì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    :param file_path: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
    :param bucket_name: S3 ë²„í‚· ì´ë¦„
    :param object_name: S3ì— ì €ì¥í•  íŒŒì¼ ì´ë¦„ (ê¸°ë³¸ì ìœ¼ë¡œ ë¡œì»¬ íŒŒì¼ ì´ë¦„ê³¼ ë™ì¼)
    :return: ì—…ë¡œë“œëœ íŒŒì¼ì˜ S3 URL
    """

    try:
        s3_client.upload_file(
            Filename=object_name,
            Bucket=bucket_name, 
            Key=f"test_user/{object_name}",
            ExtraArgs={'ContentType': 'image/png'}  # MIME íƒ€ì… ì§€ì •
        )
        
        # ì—…ë¡œë“œëœ íŒŒì¼ì˜ URL ìƒì„±
        file_url = f"https://{bucket_name}.s3.amazonaws.com/test_user/{object_name}"
        return file_url

    except Exception as e:
        print(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

############################
######### api_rag ##########
############################


def get_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (OpenAI ìµœì‹  API ì‚¬ìš©)"""
    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY)  # ìµœì‹  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        response = client.embeddings.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding  # ë³€ê²½ëœ ì¸í„°í˜ì´ìŠ¤
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return None

def search_index(index, query, top_k=3):
    """ì£¼ì–´ì§„ Pinecone ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
    query_embedding = get_embedding(query)
    if not query_embedding:
        return []
    
    result = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)
    matches = result.get("matches", []) if isinstance(result, dict) else result.matches

    return [
        match["metadata"]["chunk_text"]
        for match in matches if "metadata" in match and "chunk_text" in match["metadata"]
    ]

def generate_answer(query, relevant_texts, prompt_template):
    """LangChainì„ ì‚¬ìš©í•´ ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    context = "\n\n".join(relevant_texts)
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )
    
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            max_tokens=300,
            api_key=OPENAI_API_KEY
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        return chain.run({"context": context, "question": query})
    except Exception as e:
        print(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/rag_get_answer', methods=['POST'])
def get_answer():
    """í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ì²˜ë¦¬"""
    data = request.json
    route_num = data.get("route_num")
    query = data.get("query")

    if route_num is None or query is None:
        return jsonify({"error": "route_numê³¼ queryê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400

    if route_num == 0:
        # ë³´í—˜ ì •ë³´ ê²€ìƒ‰ ë° ë‹µë³€
        meritz_texts = search_index(index_meritz, query, top_k=3)
        samsung_texts = search_index(index_samsung, query, top_k=3)
        hanhwa_texts = search_index(index_hanhwa, query, top_k=3)
        
        relevant_texts = (
            ["[ë©”ë¦¬ì¸ ]\n" + "\n".join(meritz_texts)] +
            ["[ì‚¼ì„±í™”ì¬]\n" + "\n".join(samsung_texts)] +
            ["[í•œí™”]\n" + "\n".join(hanhwa_texts)]
        )

        prompt_template = (
            "ë‹¹ì‹ ì€ ë³´í—˜ ì¶”ì²œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ 3ê°œ ë³´í—˜ì‚¬ë¥¼ ë¹„êµí•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ 300ì ì´ë‚´ë¡œ í•´ì£¼ì„¸ìš”.\n"
            "ë¬¸ë§¥: {context}\n\n"
            "ì§ˆë¬¸: {question}\n"
            "ë‹µë³€:"
        )
    
    elif route_num == 1:
        # ë°˜ë ¤ë™ë¬¼ ì •ë³´ ê²€ìƒ‰ ë° ë‹µë³€
        diagnostic_texts = search_index(index_diagnostic, query, top_k=7)

        relevant_texts = diagnostic_texts

        prompt_template = (
            "ë‹¹ì‹ ì€ ë…¸ë ¹ê²¬ ì „ë¬¸ ì •ë³´ ì œê³µ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ 300ì ì´ë‚´ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.\n"
            "ë¬¸ë§¥: {context}\n\n"
            "ì§ˆë¬¸: {question}\n"
            "ë‹µë³€:"
        )
    
    elif route_num == 2:
        # ì¥ë¡€ì‹ì¥ ì •ë³´ ê²€ìƒ‰ ë° ë‹µë³€
        relevant_texts = [str(funeral_data)]

        prompt_template = (
            "ë‹¹ì‹ ì€ ì¥ë¡€ì‹ì¥ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì•„ë˜ì˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ íŒŒì•…í•œ í›„ ê°€ì¥ ì í•©í•œ ì¥ë¡€ì‹ì¥ì„ ë¬¸ë§¥ì—ì„œ ì°¾ì•„ ì•ˆë‚´í•´ì£¼ì„¸ìš”.\n"
            "ì„œìš¸íŠ¹ë³„ì‹œëŠ” ê²½ê¸°ë„ì™€ ë§ë‹¿ì•„ ìˆê³  ê²½ê¸°ë„ëŠ” ë™ìª½ìœ¼ë¡œ ê°•ì›ë„, ë‚¨ìª½ìœ¼ë¡œ ì¶©ì²­ë‚¨ë„, ì¶©ì²­ë¶ë„ì™€ ë§ë‹¿ì•„ ìˆìŒ \
            ì¶©ì²­ë‚¨ë„ì™€ ì¶©ì²­ë¶ë„ ì‚¬ì´ì— ëŒ€ì „ê´‘ì—­ì‹œê°€ ìˆê³ \
            ì¶©ì²­ë‚¨ë„ ë‚¨ìª½ì— ì „ë¼ë¶ë„ê°€ ìˆê³ \
            ì¶©ì²­ë¶ë„ ë™ë‚¨ìª½ì— ê²½ìƒë¶ë„ê°€ ìˆê³ \
            ì „ë¼ë¶ë„ì™€ ê²½ìƒë¶ë„ ì‚¬ì´ ë‚¨ìª½ì— ê²½ìƒë‚¨ë„ê°€ ìˆê³ \
            ì „ë¼ë¶ë„ì™€ ê²½ìƒë‚¨ë„ ì‚¬ì´ ë‚¨ì„œë°©í–¥ì— ì „ë¼ë‚¨ë„ê°€ ìˆìŒ\
            ì„œìš¸íŠ¹ë³„ì‹œëŠ” ê²½ê¸°ë„ì— ë‘˜ëŸ¬ì‹¸ì—¬ ìˆê³ \
            ì¸ì²œê´‘ì—­ì‹œëŠ” ì„œìš¸ ì„œìª½ì— ìœ„ì¹˜í•˜ë©° ê²½ê¸°ë„ì™€ ì ‘í•˜ê³  ìˆìŒ\
            ëŒ€ì „ê´‘ì—­ì‹œëŠ” ì¶©ì²­ë‚¨ë„ì™€ ì¶©ì²­ë¶ë„ ì‚¬ì´ì— ìœ„ì¹˜í•˜ê³ \
            ê´‘ì£¼ê´‘ì—­ì‹œëŠ” ì „ë¼ë‚¨ë„ì— ìœ„ì¹˜í•˜ë©° ì „ë¼ë¶ë„ì™€ ì¸ì ‘í•´ ìˆìŒ\
            ëŒ€êµ¬ê´‘ì—­ì‹œëŠ” ê²½ìƒë¶ë„ ë‚´ì— ìœ„ì¹˜í•˜ê³ \
            ë¶€ì‚°ê´‘ì—­ì‹œëŠ” ê²½ìƒë‚¨ë„ ë‚¨ë‹¨ì— ìœ„ì¹˜í•˜ë©° ë‚¨í•´ì— ì ‘í•˜ê³  ìˆìŒ\
            ìš¸ì‚°ê´‘ì—­ì‹œëŠ” ê²½ìƒë‚¨ë„ ë™ë¶ìª½ì— ìœ„ì¹˜í•˜ë©° ë™ìª½ì€ ë°”ë‹¤(ë™í•´)ì™€ ì ‘í•¨\
            "
            "ë¬¸ë§¥: {context}\n\n"
            "ì§ˆë¬¸: {question}\n"
            "ë‹µë³€:"
        )
    
    elif route_num == 3:
        answer = """ë‹¤ìŒì€ í«ë¡œìŠ¤ ì¦í›„ê·¼ ê·¹ë³µ í”„ë¡œê·¸ë¨ ë§í¬ ëª©ë¡ì…ë‹ˆë‹¤.
ë§ˆì¸ë“œì¹´í˜ ì„¼í„°
: https://center.mindcafe.co.kr/program_petloss

ë§ˆìŒì¹˜ìœ ëª¨ì„ with í«ë¡œìŠ¤
: https://www.gangnam.go.kr/contents/mind_healing/1/view.do?mid=ID04_04075401

"""

        return jsonify({"answer": answer})
    
    else:
        return jsonify({"error": "route_num ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}), 400
    answer = generate_answer(query, relevant_texts, prompt_template)
    
    return jsonify({"answer": answer})

############################
## api_letter_dreambooth ###
############################
           
def generate_letter_answer(memories, prompt, openai_api_key):
    if not memories or len(memories) < 2:  # ìµœì†Œí•œ ì„±ê²©(character)ê³¼ í’ˆì¢…(breed)ì´ ìˆì–´ì•¼ í•¨
        print("âŒ ì˜¤ë¥˜: memories ë¦¬ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ.")
        return "ê¸°ë³¸ì ì¸ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context = "\n\n".join(memories)
    
    full_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "ë¬¸ë§¥: {context}\n\n"
            "ì§ˆë¬¸: {question}\n"
            "ë‹µë³€:"
        )
    )
    
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=openai_api_key
        )
        chain = LLMChain(llm=llm, prompt=full_prompt)
        answer = chain.run({"context": context, "question": prompt})
        return answer.strip()
    except Exception as e:
        print(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

@app.route('/letter_train', methods=['POST'])
def train_dreambooth():
    data = request.json
    image_urls = data.get("images", [])

    if not image_urls:
        return jsonify({"error": "No images provided"}), 400

    # ğŸ”¹ Step 1: Download images from S3
    downloaded_images = download_s3_images(image_urls, "./train_images")

    # ğŸ”¹ Step 3: Start training only if all images are available
    command = [
        "accelerate", "launch", "--num_cpu_threads_per_process=1", TRAIN_SCRIPT,
        "--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5",
        "--instance_data_dir=./train_images",
        "--output_dir=./dreambooth_output",
        "--instance_prompt=a sks pet",
        "--resolution=512",
        "--train_batch_size=1",
        "--gradient_accumulation_steps=1",
        "--gradient_checkpointing",
        "--mixed_precision=fp16",
        "--learning_rate=5e-6",
        "--lr_scheduler=constant",
        "--lr_warmup_steps=0",
        "--max_train_steps=700",
        "--checkpointing_steps=700"
    ]

    try:
        print("ğŸš€ All images downloaded. Starting training...")
        subprocess.run(command, check=True)  # ğŸ”¹ This blocks until training completes
        print("âœ… Training completed successfully!")
        return jsonify({"message": "Training completed"}), 200
    except subprocess.CalledProcessError as e:
        print(f"âŒ Training failed: {e}")
        return jsonify({"error": "Training failed"}), 500

@app.route('/training_status', methods=['GET'])
def get_training_status():
    return jsonify(training_status), 200

@app.route('/letter_generate', methods=['POST'])
def generate_images():
    data = request.json
    character = data.get("character", "")
    breed = data.get("breed", "")
    texts = data.get("texts", [])
    memories = [character, breed] + texts

    # GPTë¡œ í¸ì§€ ìƒì„±
    letter_prompt = "ë°˜ë ¤ë™ë¬¼ì˜ ì„±ê²©ê³¼ ë°˜ë ¤ë™ë¬¼ê³¼ì˜ ì¶”ì–µì„ ê¸°ë¡í•œ ê²Œì‹œê¸€ì„ ë°”íƒ•ìœ¼ë¡œ \
        ë°˜ë ¤ë™ë¬¼ì´ ì£¼ì¸ì—ê²Œ ì“°ëŠ” ë”°ëœ»í•œ í¸ì§€ë¥¼ ë°˜ë§ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
    letter = generate_letter_answer(memories, letter_prompt, OPENAI_API_KEY )
    
    # GPTë¡œ DreamBooth í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
    prompt_extraction = "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ DreamBooth ëª¨ë¸ì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ì•„ì£¼ ì§§ê²Œ ìƒì„±í•˜ì„¸ìš”.\
        ì–´ë–¤ ìƒí™©ì„ ë¬˜ì‚¬í•˜ëŠ” ë‚´ìš©ì´ë©° 'a sks ...' í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.\
        (ex) a sks cat on a grass"
    dreambooth_prompt = generate_letter_answer(memories, prompt_extraction, OPENAI_API_KEY )
    dreambooth_prompt = "high quality, J_illustration, " + dreambooth_prompt
    
    print(dreambooth_prompt)
    
    checkpoint_dir = "./dreambooth_output/checkpoint-700"
    unet = UNet2DConditionModel.from_pretrained(
        os.path.join(checkpoint_dir, "unet"),
        torch_dtype=torch.float16,
        local_files_only=True
    ).to(device)

    pipeline = DiffusionPipeline.from_pretrained(
        MODEL_NAME,
        unet=unet,
        torch_dtype=torch.float16
    ).to(device)
    
    lora_path = "./J_illustration.safetensors"
    pipeline.load_lora_weights(lora_path)

    # ì´ë¯¸ì§€ ìƒì„±
    guidance_scales = [5, 6, 7, 8, 9, 10]
    inference_steps = [100]
    generated_images = []
    
    for scale in guidance_scales:
        for step in inference_steps:
            with torch.autocast(device.type):
                result = pipeline(dreambooth_prompt, num_inference_steps=step, guidance_scale=scale)
            generated_images.append(result.images[0])

    # ìµœì¢… ì´ë¯¸ì§€ 6ì¥ ì„ íƒ
    encoded_images = []
    for idx, image in enumerate(generated_images[:6]):
        local_path = f"generated_image_{idx}.png"
        
        # ì´ë¯¸ì§€ ì €ì¥ (PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ PNG í˜•ì‹ìœ¼ë¡œ ì €ì¥)
        image.save(local_path, format="PNG")
        print(f"âœ… Image saved locally: {local_path}")

        # S3 ì—…ë¡œë“œ
        # object_name = f"generated_image_{idx}.png"
        file_url = upload_svg_to_s3(BUCKET_NAME, local_path)  # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©

        if file_url:
            encoded_images.append(file_url)
            print(f"âœ… Uploaded to S3: {file_url}")
        else:
            print(f"âŒ Failed to upload {local_path} to S3")

    shutil.rmtree("./dreambooth_output", ignore_errors=True)
    shutil.rmtree("./train_images", ignore_errors=True)

    return jsonify({"images": encoded_images, "letter": letter})

############################
######### api_stars ########
############################

def process_segmentation(image, mask, edge_threshold1=50, edge_threshold2=150,
                         scale_factor=0.9, mask_threshold=30, max_internal_points=1000):
    """
    1) ë§ˆìŠ¤í¬ì—ì„œ ê°€ì¥ í° ìœ¤ê³½ì„ ì„ ì°¾ì•„ ì¼ì • ê°„ê²©ìœ¼ë¡œ ì ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    2) ë§ˆìŠ¤í¬ ë‚´ë¶€ ì˜ì—­ì— ëŒ€í•´ Canny ì—ì§€ ê²€ì¶œ í›„ ì—ì§€ ì ë“¤ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
    3) ë‘ ì¢…ë¥˜ì˜ ì (ìœ¤ê³½ì„ , ë‚´ë¶€ ì—ì§€)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if mask.sum() == 0:
        raise ValueError("The mask is empty. Check the input image or segmentation result.")

    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ (ì»¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš°)
    if len(image.shape) == 3:
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray_image = image

    # ë§ˆìŠ¤í¬ ì´ì§„í™”
    _, mask_uint8 = cv2.threshold((mask * 255).astype(np.uint8), mask_threshold, 255, cv2.THRESH_BINARY)
    
    # ìœ¤ê³½ì„  ì¶”ì¶œ
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        raise ValueError("No contours found. Check the mask content.")

    # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
    largest_contour = max(contours, key=cv2.contourArea)

    # ìœ¤ê³½ì„  ê·¼ì‚¬
    epsilon = 0.01 * cv2.arcLength(largest_contour, True)
    approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)
    contour_points = approx_contour[:, 0, :]
    if len(contour_points) < 50:
        contour_points = largest_contour[:, 0, :]

    # ì  ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°í™” ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŒ)
    step = max(1, len(contour_points) // 60)
    contour_points = contour_points[::step]

    # ë‚´ë¶€ ì˜ì—­ ë§ˆìŠ¤í¬ (scale_factor ë§Œí¼ ì¤„ì¸ í›„ ì¤‘ì•™ ì •ë ¬)
    scaled_mask = cv2.resize(mask_uint8, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)
    dh = (mask.shape[0] - scaled_mask.shape[0]) // 2
    dw = (mask.shape[1] - scaled_mask.shape[1]) // 2
    scaled_mask_padded = cv2.copyMakeBorder(
        scaled_mask,
        top=dh,
        bottom=(mask.shape[0] - scaled_mask.shape[0] - dh),
        left=dw,
        right=(mask.shape[1] - scaled_mask.shape[1] - dw),
        borderType=cv2.BORDER_CONSTANT,
        value=0
    )

    # Canny ì—£ì§€ ê²€ì¶œ
    edges = cv2.Canny(gray_image, edge_threshold1, edge_threshold2)
    # ë‚´ë¶€ ì˜ì—­ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì—ì§€ ì œê±°
    edges[scaled_mask_padded == 0] = 0

    # ì—ì§€ ì  ì¶”ì¶œ
    edge_y, edge_x = np.where(edges > 0)
    edge_points = np.stack((edge_x, edge_y), axis=-1)
    num_points = edge_points.shape[0]
    
    # ìµœëŒ€ max_internal_points ê°œë¡œ ìƒ˜í”Œë§ (ë” ë§ì´ ë½‘íˆë„ë¡ 500 -> max_internal_points)
    num_sample = min(num_points, max_internal_points)
    if num_points > 0:
        random_indices = np.random.choice(num_points, num_sample, replace=False)
        random_edge_points = edge_points[random_indices]
    else:
        random_edge_points = np.empty((0, 2), dtype=int)

    return contour_points, random_edge_points
    
def sample_pidinet_edges(pidinet_output, mask, scale_factor=0.9, sample_size=1000):
    """
    pidinet ê²°ê³¼ì—ì„œ í°ìƒ‰(ì „ê²½) ë¶€ë¶„ì„ ìƒ˜í”Œë§í•˜ëŠ” í•¨ìˆ˜
    1) ë§ˆìŠ¤í¬ì—ì„œ ê°€ì¥ í° ìœ¤ê³½ì„ ì„ ì°¾ì•„ ì¼ì • ê°„ê²©ìœ¼ë¡œ ì ë“¤ì„ ì¶”ì¶œ
    2) pidinet ì¶œë ¥ì—ì„œ ë§ˆìŠ¤í¬ ë‚´ë¶€ì˜ í°ìƒ‰(ì „ê²½) ì ë“¤ì„ ìƒ˜í”Œë§
    """
    if mask.sum() == 0:
        raise ValueError("The mask is empty. Check the input image or segmentation result.")
    
    # pidinet_outputì„ numpy ë°°ì—´ë¡œ ë³€í™˜
    if not isinstance(pidinet_output, np.ndarray):
        pidinet_output = np.array(pidinet_output.convert("L"))  # í‘ë°± ë³€í™˜
    
    # ë§ˆìŠ¤í¬ ì´ì§„í™”
    _, mask_uint8 = cv2.threshold((mask * 255).astype(np.uint8), 30, 255, cv2.THRESH_BINARY)
    
    # ìœ¤ê³½ì„  ì¶”ì¶œ
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        raise ValueError("No contours found. Check the mask content.")
    
    # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
    largest_contour = max(contours, key=cv2.contourArea)
    
    # ìœ¤ê³½ì„  ê·¼ì‚¬í™” ë° ìƒ˜í”Œë§
    epsilon = 0.01 * cv2.arcLength(largest_contour, True)
    approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)
    contour_points = approx_contour[:, 0, :]
    
    if len(contour_points) < 50:
        contour_points = largest_contour[:, 0, :]
    
    step = max(1, len(contour_points) // 60)
    contour_points = contour_points[::step]
    
    # ë‚´ë¶€ ì˜ì—­ ë§ˆìŠ¤í¬(scale_factor ë§Œí¼ ì¶•ì†Œ)
    scaled_mask = cv2.resize(mask_uint8, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)
    dh = (mask.shape[0] - scaled_mask.shape[0]) // 2
    dw = (mask.shape[1] - scaled_mask.shape[1]) // 2
    scaled_mask_padded = cv2.copyMakeBorder(
        scaled_mask, dh, mask.shape[0] - scaled_mask.shape[0] - dh, 
        dw, mask.shape[1] - scaled_mask.shape[1] - dw, 
        borderType=cv2.BORDER_CONSTANT, value=0
    )

    # ê¸°ë³¸ threshold ê°’ ë° ì„¤ì •
    base_threshold = 0
    min_threshold = 5
    max_threshold = 40
    target_ratio = 0.05  # ëª©í‘œ í°ìƒ‰ ë¹„ìœ¨
    scaling_factor = 10  # ë³€í™”ëŸ‰ ì¡°ì ˆ

    # ì „ì²´ í”½ì…€ì—ì„œ í°ìƒ‰ ë¹„ìœ¨ ê³„ì‚°
    total_pixels = np.sum(scaled_mask_padded > 0)  # ë§ˆìŠ¤í¬ ë‚´ë¶€ì˜ ìœ íš¨ í”½ì…€ ê°œìˆ˜
    edge_pixels = np.sum(pidinet_output > base_threshold)  # ì´ˆê¸° thresholdì—ì„œ ê²€ì¶œëœ í”½ì…€ ê°œìˆ˜
    white_ratio = edge_pixels / total_pixels if total_pixels > 0 else 0  # í°ìƒ‰ ë¹„ìœ¨

    # ì„ í˜• ìŠ¤ì¼€ì¼ë§ì„ í™œìš©í•œ ë™ì  threshold ê³„ì‚°
    dynamic_threshold = base_threshold + scaling_factor * (white_ratio - target_ratio)
    # dynamic_threshold = np.clip(dynamic_threshold, min_threshold, max_threshold)  # min~max ì œí•œ/
    dynamic_threshold = 15

    # ìƒˆë¡œìš´ thresholdë¡œ í°ìƒ‰ í”½ì…€ ì°¾ê¸°
    edge_y, edge_x = np.where((pidinet_output > dynamic_threshold) & (scaled_mask_padded > 0))
    edge_points = np.stack((edge_x, edge_y), axis=-1)

    print(f"White Ratio: {white_ratio:.4f}, Adjusted Threshold: {dynamic_threshold:.2f}")
    
    # ìƒ˜í”Œë§ ìˆ˜í–‰ (ë§ˆìŠ¤í¬ ë‚´ë¶€ì—ì„œ ì„ íƒ)
    num_points = edge_points.shape[0]
    num_sample = min(num_points, sample_size)
    if num_points > 0:
        random_indices = np.random.choice(num_points, num_sample, replace=False)
        random_edge_points = edge_points[random_indices]
    else:
        random_edge_points = np.empty((0, 2), dtype=int)
    
    return contour_points, random_edge_points

def load_and_preprocess_image(image_path):
    """ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {image_path}")
    
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image, image_rgb

def run_sam_segmentation(image_rgb, predictor, point):
    """SAM ëª¨ë¸ì„ ì´ìš©í•´ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ ìˆ˜í–‰"""
    predictor.set_image(image_rgb)
    
    h, w, _ = image_rgb.shape
    point = np.array([point])
    input_label = np.array([1])  # 1: object

    masks, scores, _ = predictor.predict(
        point_coords=point,
        point_labels=input_label,
        multimask_output=True,
    )

    best_mask_index = int(np.argmax(scores))
    return masks[best_mask_index]

def extract_and_sort_centroids(contour_points, n_clusters, n_points):
    """
    ì£¼ì–´ì§„ ì»¨íˆ¬ì–´ í¬ì¸íŠ¸ì—ì„œ K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ 
    n_points ê°œì˜ ì¤‘ì‹¬ì ì„ ëœë¤í•˜ê²Œ ì„ íƒí•œ í›„, ê°€ì¥ ê°€ê¹Œìš´ ì ë“¤ë¡œ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜.

    :param contour_points: (N, 2) í˜•íƒœì˜ numpy ë°°ì—´, ì»¨íˆ¬ì–´ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
    :param n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    :param n_points: ì„ íƒí•  ì¤‘ì‹¬ì  ê°œìˆ˜
    :return: ìµœê·¼ì ‘ ìˆœì„œë¡œ ì •ë ¬ëœ ì¤‘ì‹¬ì  ë¦¬ìŠ¤íŠ¸
    """
    if len(contour_points) < n_clusters:
        raise ValueError("Contour points ê°œìˆ˜ê°€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë³´ë‹¤ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.")

    # K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(contour_points)

    # ì¤‘ì‹¬ì (centroids) ê°€ì ¸ì˜¤ê¸°
    centroids = kmeans.cluster_centers_

    # ì¤‘ì‹¬ì  ì¤‘ n_points ê°œ ëœë¤ ì„ íƒ
    num_selected = min(n_points, n_clusters)
    selected_centroids = np.array(random.sample(centroids.tolist(), num_selected))

    # ìµœê·¼ì ‘ ì´ì›ƒ ë°©ì‹ìœ¼ë¡œ ì •ë ¬
    sorted_centroids = nearest_neighbor_sort(selected_centroids)

    return sorted_centroids

def nearest_neighbor_sort(points):
    """
    ìµœê·¼ì ‘ ì´ì›ƒ(Nearest Neighbor) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì ë“¤ì„ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜.

    :param points: (N, 2) í˜•íƒœì˜ numpy ë°°ì—´, ëœë¤ ì„ íƒëœ ì¤‘ì‹¬ì  ë¦¬ìŠ¤íŠ¸
    :return: ìµœê·¼ì ‘ ìˆœì„œë¡œ ì •ë ¬ëœ numpy ë°°ì—´
    """
    points = points.tolist()
    sorted_points = [points.pop(0)]  # ì²« ë²ˆì§¸ ì ì„ ì‹œì‘ì ìœ¼ë¡œ ì„¤ì •

    while points:
        last_point = sorted_points[-1]
        nearest_point = min(points, key=lambda p: np.linalg.norm(np.array(p) - np.array(last_point)))
        sorted_points.append(nearest_point)
        points.remove(nearest_point)

    return np.array(sorted_points)

def generate_mst_graph(major_points):
    """MSTë¥¼ ìƒì„±í•˜ê³  ì—°ê²°ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜"""
    knn_graph = kneighbors_graph(major_points, n_neighbors=3, mode='distance')
    mst_matrix = minimum_spanning_tree(knn_graph)
    coo = mst_matrix.tocoo()
    edges = np.vstack((coo.row, coo.col)).T

    n_components, labels = connected_components(mst_matrix)
    if n_components > 1:
        distance_matrix = pairwise_distances(major_points)
        added_edges = []

        for i in range(n_components - 1):
            for j in range(i + 1, n_components):
                mask_i, mask_j = labels == i, labels == j
                min_dist, min_edge = np.inf, None

                for idx_i in np.where(mask_i)[0]:
                    for idx_j in np.where(mask_j)[0]:
                        if distance_matrix[idx_i, idx_j] < min_dist:
                            min_dist = distance_matrix[idx_i, idx_j]
                            min_edge = (idx_i, idx_j)

                if min_edge:
                    added_edges.append(min_edge)

        edges = np.vstack((edges, added_edges))
    
    return edges

def image_to_svg(image, svg_path, threshold=128):
    """ì´ë¯¸ì§€ë¥¼ SVG í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    height, width = image.shape
    dwg = svgwrite.Drawing(svg_path, size=(width, height))

    for y in range(height):
        for x in range(width):
            if image[y, x] >= threshold:
                dwg.add(dwg.circle(center=(x, y), r=0.5, fill='gray', stroke='none'))
    
    dwg.save()

def enhance_contrast(image):
    """Convert image to grayscale and enhance contrast using CLAHE"""
    if len(image.shape) == 3:  # Convert to grayscale if the image has multiple channels
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()

    # Ensure the image is 8-bit (uint8)
    gray = np.clip(gray, 0, 255).astype(np.uint8)

    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    return enhanced

@app.route("/stars_run_pidinet", methods=["POST"])
def run_pidinet():
    """
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ PiDiNet ì‹¤í–‰.
    """
    data = request.json
    print(f"ğŸ“¥ Received Data: {data}")  # Debugging log

    image_url = data.get("image_url")

    if not image_url:
        return jsonify({"error": "No images provided"}), 400

    # ë‹¨ì¼ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(image_url, str):
        image_url = [image_url]
        
    stars_download_s3_images(image_urls = image_url, save_folder="./img")
    
    # ğŸ”¹ Step 3: PiDiNet ì‹¤í–‰ ëª…ë ¹ì–´
    command = [
        "python", "pidinet-master/main.py",
        "--model", "pidinet_converted",
        "--config", "carv4",
        "--sa", "--dil",
        "-j", "4",
        "--gpu", "0",
        "--resume",
        "--savedir", "./img_edges",
        "--datadir", "./img",
        "--dataset", "Custom",
        "--evaluate", "./table5_pidinet.pth",
        "--evaluate-converted"
    ]

    try:
        print("ğŸš€ PiDiNet ì‹¤í–‰ ì¤‘...")
        result = subprocess.run(command, check=True)  # ì‹¤í–‰ (ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°)
        print("âœ… PiDiNet ì‹¤í–‰ ì™„ë£Œ!")

        return jsonify({
            "message": "PiDiNet execution completed",
        }), 200

    except subprocess.CalledProcessError as e:
        print(f"âŒ PiDiNet ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return jsonify({"error": "PiDiNet execution failed"}), 500

@app.route("/stars_process_image", methods=["POST"])
def process_image():
    """
    í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ URLê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ë°›ì•„ ì²˜ë¦¬.
    """
    data = request.json
    image_url = data.get("image_url")
    point = data.get("point")

    if not image_url:
        return jsonify({"error": "No image URL provided"}), 400
    
    image_name = os.path.basename(image_url)
    image_path = "./img/" + image_name
    
    image, image_rgb = load_and_preprocess_image(image_path)
    mask = run_sam_segmentation(image_rgb, predictor, point)
    
    mask_ratio = np.count_nonzero(mask) / mask.size
    max_internal_points = int(mask_ratio * 3000)
    
    contour_points2, internal_points2 = process_segmentation(image, mask, max_internal_points=max_internal_points)

    image_path = f"./img_edges/eval_results/imgs_epoch_019/{os.path.splitext(image_name)[0]}.png"
    image = Image.open(image_path)
    image = np.array(image.convert("L"))
    
    masked_image = np.zeros_like(image, dtype=np.float32) 
    masked_image[mask > 0] = image[mask > 0] 
    processed_image = enhance_contrast(masked_image)
    
    svg_path = f"{os.path.splitext(image_name)[0]}.svg"
    png_path = f"{os.path.splitext(image_name)[0]}_masked.png"
    
    image_to_svg(processed_image, svg_path)
    cairosvg.svg2png(url=svg_path, write_to=png_path)
    
    try:
        svg_path = upload_svg_to_s3(BUCKET_NAME,f"{os.path.splitext(image_name)[0]}.svg")
    except Exception as e:
        print(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    contour_points1, internal_points1 = sample_pidinet_edges(image, mask, sample_size=max_internal_points)
    
    internal_points = np.vstack((internal_points1, internal_points2))
    contour_points = np.vstack((contour_points2))

    major_contour = extract_and_sort_centroids(contour_points, n_clusters=20, n_points=5)
    major_internal = extract_and_sort_centroids(internal_points, n_clusters=20, n_points=10)
    
    major_points = np.vstack((major_contour, major_internal))
    edges = generate_mst_graph(major_points)

    return jsonify({
        "svg_path": svg_path,
        "edges": edges.tolist(),  # Convert NumPy arrays to lists
        "major_points": major_points.astype(int).tolist()
    })



if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)